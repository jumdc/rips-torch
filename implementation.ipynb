{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check implementation with Guhdi's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gudhi\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from ripslayer import RipsModule\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small example\n",
    "Analogy between the tf and torch's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudhi.tensorflow.rips_layer import RipsLayer\n",
    "from gudhi.wasserstein import wasserstein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "angles = np.random.uniform(0,2*np.pi,100)\n",
    "X = np.hstack([ np.cos(angles)[:,None], np.sin(angles)[:,None] ])\n",
    "dim = 1\n",
    "X = np.array([[0.1,0.],[1.5,1.5],[0.,1.6]])\n",
    "dim = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTF = tf.Variable(X, dtype=tf.float32)\n",
    "lr = 1\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "num_epochs = 1\n",
    "losses, Dgs, Xs, grads = [], [], [], []\n",
    "for epoch in range(num_epochs+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer = RipsLayer(homology_dimensions=[dim], maximum_edge_length=10)\n",
    "        dgm = layer.call(X=XTF)[0][0]\n",
    "        loss = - wasserstein_distance(dgm, tf.constant(np.empty([0,2])), order=1, enable_autodiff=True)\n",
    "    Dgs.append(dgm.numpy())            \n",
    "    Xs.append(XTF.numpy())\n",
    "    losses.append(loss.numpy())\n",
    "    gradients = tape.gradient(loss, [XTF])\n",
    "    grads.append(gradients[0].numpy())\n",
    "    optimizer.apply_gradients(zip(gradients, [XTF]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_to_move = np.argwhere(np.linalg.norm(grads[0], axis=1) != 0).ravel()\n",
    "plt.figure()\n",
    "for pt in pts_to_move:\n",
    "    plt.arrow(Xs[0][pt,0], Xs[0][pt,1], -lr*grads[0][pt,0], -lr*grads[0][pt,1], color='blue',\n",
    "              length_includes_head=True, head_length=.05, head_width=.1, zorder=10)\n",
    "plt.scatter(Xs[0][:,0], Xs[0][:,1], c='red', s=50, alpha=.2,  zorder=3)\n",
    "plt.scatter(Xs[0][pts_to_move,0], Xs[0][pts_to_move,1], c='red',   s=150, marker='o', zorder=2, alpha=.7, label='Step i')\n",
    "plt.scatter(Xs[1][pts_to_move,0], Xs[1][pts_to_move,1], c='green', s=150, marker='o', zorder=1, alpha=.7, label='Step i+1')\n",
    "plt.axis('square')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = nn.Parameter(torch.tensor(X, dtype=torch.float32), requires_grad=True)\n",
    "lr = 1\n",
    "optimizer = torch.optim.SGD([X_torch], lr=lr)\n",
    "optimizer.zero_grad()\n",
    "num_epochs = 1\n",
    "losses, Dgs, Xs, grads = [], [], [], []\n",
    "rips = RipsModule(homology_dimensions=[dim], maximum_edge_length=10)\n",
    "for epoch in range(num_epochs+1):\n",
    "    diag = rips(X_torch)[0][0]\n",
    "    loss = - wasserstein_distance(diag, torch.empty([0,2]), order=1, enable_autodiff=True)\n",
    "    Dgs.append(diag.detach().numpy())\n",
    "    Xs.append(XTF.detach().numpy())\n",
    "    losses.append(loss.detach().numpy())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_to_move = np.argwhere(np.linalg.norm(grads[0], axis=1) != 0).ravel()\n",
    "plt.figure()\n",
    "for pt in pts_to_move:\n",
    "    plt.arrow(Xs[0][pt,0], Xs[0][pt,1], -lr*grads[0][pt,0], -lr*grads[0][pt,1], color='blue',\n",
    "              length_includes_head=True, head_length=.05, head_width=.1, zorder=10)\n",
    "plt.scatter(Xs[0][:,0], Xs[0][:,1], c='red', s=50, alpha=.2,  zorder=3)\n",
    "plt.scatter(Xs[0][pts_to_move,0], Xs[0][pts_to_move,1], c='red',   s=150, marker='o', zorder=2, alpha=.7, label='Step i')\n",
    "plt.scatter(Xs[1][pts_to_move,0], Xs[1][pts_to_move,1], c='green', s=150, marker='o', zorder=1, alpha=.7, label='Step i+1')\n",
    "plt.axis('square')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difftda",
   "language": "python",
   "name": "difftda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
